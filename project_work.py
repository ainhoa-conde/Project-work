# -*- coding: utf-8 -*-
"""Project work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/191iQvVXDqlfNbQroE9N8jLjE9ziKAIrH

#Version 1

##Data cleaning and selection

After manually separating the patients that have the smartwatch data, now it's time to clean and select the data that we have.

There are some points to be taken into account for this, for example, the patients need to have at least 5 hours of data in order to be fit for the study.

We also need to add data from other datasets that determine things such as the sex and age of the patient, as well as if they have any simialr patologies in their family.
"""

import pandas as pd
import os

from google.colab import drive
drive.mount('/content/drive')

path_info = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/'
file_info = 'informacion_pacientes.csv'
#selecting the columns necessary for the study
columns = ["DNI", "Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]
info = pd.read_csv(path_info + file_info, usecols = columns,encoding="utf-8", sep = ";")

#change the sex column to binary
info["Sexo"] = info["Sexo"].map({"Hombre": 0, "Mujer": 1})

info.head()

path_vari = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/'
file_vari = 'variables-psg.csv'
#selecting the columns necessary for the study
columns_2 = ["DNI", "Tiempo total de sueño -TTS- (minutos):"]
vari = pd.read_csv(path_vari + file_vari, usecols = columns_2, encoding="utf-8", sep = ";")

#checking that the patient has had at least 5 hours of sleep
vari = vari[vari["Tiempo total de sueño -TTS- (minutos):"] > 300]

vari.head()

#merging both datasets so they have the same patients
data_combined = pd.merge(info, vari, on="DNI", how="inner")

#taking out the lines where the patient has no individual csv with their own data
no_data = ["who_006", "who_007", "who_008", "who_012", "who_014", "who_016", "who_019", "who_030", "who_031", "who_033", "who_035", "who_036", "who_037"]
data_combined = data_combined[~data_combined["DNI"].isin(no_data)]

data_combined.head()

#check if the patients have more tahn 5 hours of sleep based on the first and last timestamps
from datetime import datetime, timedelta
all_patients = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes'

patients = set(data_combined['DNI'])

valid_patients = []

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check if the patient really has 5 hours of sleep
      first_timestamp = data_patient[" timestamp"].iloc[0]
      last_timestamp = data_patient[" timestamp"].iloc[-1]

      first_timestamp = datetime.strptime(first_timestamp, "%H_%M_%S")
      last_timestamp = datetime.strptime(last_timestamp, "%H_%M_%S")

      #first_seconds = int(first_timestamp[0]) * 3600 + int(first_timestamp[1]) * 60 + int(first_timestamp[2])
      #last_seconds = int(last_timestamp[0]) * 3600 + int(last_timestamp[1]) * 60 + int(last_timestamp[2])

      # since the last time is lower than the first one, they have to be substracted the other way around
      if last_timestamp < first_timestamp:  # Caso en que pasa de medianoche
        last_timestamp += timedelta(days=1)

      duration = last_timestamp - first_timestamp

      cutoff = first_timestamp + timedelta(minutes=30)
      data_patient['timestamp'] = data_patient[" timestamp"].apply(lambda x: datetime.strptime(x, "%H_%M_%S"))
      data_patient = data_patient[data_patient['timestamp'] >= cutoff]

      if duration.total_seconds() / 3600 >= 5:
        valid_patients.append(patient_nb)

"""Since some patients have more than one file, we merge them together to have the data from the whole night."""

#creating a csv for each patient with their bmi, age and sex and saving them on specific patient folders
base_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados/'
bas = ["Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]

for index, row in data_combined.iterrows():
  patient_dni = row['DNI'].lower()
  if patient_dni in valid_patients:
    folder_name = f"{base_folder}_{row['DNI']}"

    if not os.path.exists(folder_name):
      os.makedirs(folder_name)

    row_data = pd.DataFrame([row[bas]])
    file_name = f"{folder_name}/{row['DNI']}_BMI.csv"
    row_data.to_csv(file_name, index = False)

path_005 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_005/'
all_data_005 = os.listdir(path_005)
save_to = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'

who_005 = []
for csvd in all_data_005:
    df = pd.read_csv(path_005 + csvd, encoding="utf-8", sep = ",")
    who_005.append(df)

patient_005 = pd.concat(who_005, ignore_index=True)
output_file = os.path.join(save_to, "WHO_005_data.csv")
patient_005.to_csv(output_file, index=False, encoding="utf-8")

path_027 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_027/'
all_data_027 = os.listdir(path_027)

who_027 = []
for csvd in all_data_027:
    df = pd.read_csv(path_027 + csvd, encoding="utf-8", sep = ",")
    who_027.append(df)

patient_027 = pd.concat(who_027, ignore_index=True)
output_file = os.path.join(save_to, "WHO_027_data.csv")
patient_027.to_csv(output_file, index=False, encoding="utf-8")

"""Now we have to separate the HR, IBI, temperature and SPO2 by the patient and save each of them on their own csv file. All of these have to have the timestamps.

Note: column names have a space in the beginning

Index([' timestamp', ' hr', ' ibi', ' sop2', ' temp', ' ambientT', ' acx',
       ' acy', ' acz', ' sensor '],
      dtype='object')
"""

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados/'

hr = [" timestamp", " hr"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the hr
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the hr and timestamp
      patient_hr = data_patient[hr]

      #eliminate all values that are 0
      patient_hr = patient_hr[patient_hr[" hr"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_HR.csv")
      patient_hr.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados/'

ibi = [" timestamp", " ibi"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the ibi
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the ibi and timestamp
      patient_ibi = data_patient[ibi]

      #eliminate all values that are 0
      patient_ibi = patient_ibi[patient_ibi[" ibi"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_IBI.csv")
      patient_ibi.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados/'

temp = [" timestamp", " temp"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the temp
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "T"]

      #look just for the temp and timestamp
      patient_temp = data_patient[temp]

      #eliminate all values that are 0
      patient_temp = patient_temp[patient_temp[" temp"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_TEMP.csv")
      patient_temp.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados/'

sop2 = [" timestamp", " sop2"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the sop2
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "S"]

      #look just for the sop2 and timestamp
      patient_sop2 = data_patient[sop2]

      #eliminate all values that are 0
      patient_sop2 = patient_sop2[patient_sop2[" sop2"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_SOP2.csv")
      patient_sop2.to_csv(output_file, index = False)

"""##Classification

Create an array for each patient. This array will contain the following:

- Sex (bianry number, 0 for male, 1 for female)
- BMI
- mean_ibi, median_ibi, std_ibi, mode_ibi
- mean_hr, median_hr, std_hr, mode_hr
- mean_temp, median_temp, std_temp, mode_temp
- mean_sop2, median_sop2, std_sop2, mode_sop2
"""

import os
import pandas as pd
import math

patinet_files_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados'
output_fold = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa'

all_patients_data = []

columns = ['DNI', 'sex', 'bmi', 'mean_hr', 'median_hr', 'std_hr', 'mode_hr',
           'mean_ibi', 'median_ibi', 'std_ibi', 'mode_ibi',
           'mean_sop2', 'median_sop2', 'std_sop2', 'mode_sop2',
           'mean_temp', 'median_temp', 'std_temp', 'mode_temp']

#round rumbers when dividing the number of lines
def safe_round(value):
    if pd.isna(value):
        return 0  #if the number is NA, change it to 0
    return round(value)

#function to divide in 4 groups
def divide_in_groups(df, group_size):
    #round to lower value
    n_groups = math.floor(len(df) / group_size)
    groups = []

    #create new groups
    for i in range(n_groups):
        start = i * group_size
        end = start + group_size
        group = df.iloc[start:end]
        groups.append(group)

    #add rows if they are not divisible by 4
    if len(df) % group_size != 0:
        group = df.iloc[n_groups * group_size:]
        groups.append(group)

    return groups

#go throught each patient
for patient_folder in os.listdir(patinet_files_path):
    folder_path = os.path.join(patinet_files_path, patient_folder)

    if os.path.isdir(folder_path):
        #get rid of 'who_' temporally to check the names properly
        patient_id = patient_folder[1:]

        data_array = [None] * len(columns)
        data_array[0] = patient_id

        new_patient_data = []

        for data_file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, data_file)

            if data_file.endswith("BMI.csv"):
                bmi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                data_array[1] = bmi.iloc[0]["Sexo"]
                data_array[2] = safe_round(bmi.iloc[0]["Indice de masa corporal (peso en Kg/talla en metros²)"])

            elif data_file.endswith("HR.csv"):
                hr = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(hr) / 4)
                hr_groups = divide_in_groups(hr, group_size)

                for i, group in enumerate(hr_groups, 1):
                    new_patient = data_array.copy()
                    #add '_0X' to the end of the DNI column
                    new_patient[0] = f"{patient_id}_{str(i).zfill(2)}"
                    new_patient[3] = safe_round(group[" hr"].mean())
                    new_patient[4] = safe_round(group[" hr"].median())
                    new_patient[5] = safe_round(group[" hr"].std())
                    mode_value = round(group[" hr"].mode())
                    new_patient[6] = safe_round(mode_value.iloc[0]) if not mode_value.empty else None
                    new_patient_data.append(new_patient)

            elif data_file.endswith("IBI.csv"):
                ibi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(ibi) / 4)
                ibi_groups = divide_in_groups(ibi, group_size)

                for i, group in enumerate(ibi_groups, 1):
                    new_patient = data_array.copy()
                    new_patient[0] = f"{patient_id}_{str(i).zfill(2)}"
                    new_patient[7] = safe_round(group[" ibi"].mean())
                    new_patient[8] = safe_round(group[" ibi"].median())
                    new_patient[9] = safe_round(group[" ibi"].std())
                    mode_value = round(group[" ibi"].mode())
                    new_patient[10] = safe_round(mode_value.iloc[0]) if not mode_value.empty else None
                    new_patient_data.append(new_patient)

            elif data_file.endswith("SOP2.csv"):
                sop2 = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(sop2) / 4)
                sop2_groups = divide_in_groups(sop2, group_size)

                for i, group in enumerate(sop2_groups, 1):
                    new_patient = data_array.copy()
                    new_patient[0] = f"{patient_id}_{str(i).zfill(2)}"
                    new_patient[11] = safe_round(group[" sop2"].mean())
                    new_patient[12] = safe_round(group[" sop2"].median())
                    new_patient[13] = safe_round(group[" sop2"].std())
                    mode_value = round(group[" sop2"].mode())
                    new_patient[14] = safe_round(mode_value.iloc[0]) if not mode_value.empty else None
                    new_patient_data.append(new_patient)

            elif data_file.endswith("TEMP.csv"):
                temp = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(temp) / 4)
                temp_groups = divide_in_groups(temp, group_size)

                for i, group in enumerate(temp_groups, 1):
                    new_patient = data_array.copy()
                    new_patient[0] = f"{patient_id}_{str(i).zfill(2)}"
                    new_patient[15] = safe_round(group[" temp"].mean())
                    new_patient[16] = safe_round(group[" temp"].median())
                    new_patient[17] = safe_round(group[" temp"].std())
                    mode_value = round(group[" temp"].mode())
                    new_patient[18] = safe_round(mode_value.iloc[0]) if not mode_value.empty else None
                    new_patient_data.append(new_patient)

        all_patients_data.extend(new_patient_data)

#save data
df = pd.DataFrame(all_patients_data, columns=columns)
df = df.groupby('DNI').sum().reset_index()
df = df[~df['DNI'].str.endswith('_05')]
output_csv = os.path.join(output_fold, 'classification_data.csv')
df.to_csv(output_csv, index=False)
df.head(10)

"""###There are 4 groups in which each patient can be put into depending on their IAH:

    * Normal, there is no sleep apnea-hypopnea syndrome: IAH <5.
    * Mild sleep apnoea syndrome: IAH 5-14,9.
    * Moderate sleep apnea syndrome: IAH: 15-29,9.
    * Severe sleep apnoea syndrome: IAH >30.
"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn import tree

iahs_file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/variables-psg.csv'
iahs_data = pd.read_csv(iahs_file_path, encoding="utf-8", sep=";")

file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data.csv'
df = pd.read_csv(file_path)

#since we separated the patients in different rows, we need to have the same name as the DNI column in iahs_data
df['DNI_temp'] = df['DNI']
df['DNI'] = df['DNI'].str.extract(r'(^who_\d+)')

#assign classes based on types
def assign_class(IAH):
    if IAH < 5:
        return 'No apnea'
    elif 5 <= IAH < 15:
        return 'Mild'
    elif 15 <= IAH < 30:
        return 'Moderate'
    else:
        return 'Severe'

iahs_data['Class'] = iahs_data['Indice de apneas hipopneas por hora (IAH)'].apply(assign_class)

#merge data
merge_data = df.merge(iahs_data[['DNI', 'Class']], on='DNI', how='inner')

#put back the names of the different patients
merge_data['DNI'] = merge_data['DNI'] + '_' + merge_data['DNI_temp'].str.split('_').str[2]

#delete temp column
merge_data.drop(columns=['DNI_temp'], inplace=True)
merge_data

#checking how many patientshave each type of apnea
unique_patients_in_class = merge_data.drop_duplicates(subset=['DNI'])['Class'].value_counts()

#dividing by 4 because there are 4 rows for each patient
print(unique_patients_in_class/4)

#change apnea classes to numeric
class_mapping = {'No apnea': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3}
merge_data['Class'] = merge_data['Class'].map(class_mapping)

X = merge_data.drop(columns=['Class', 'DNI'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

print(pd.Series(y_pred).value_counts())

#gini impurity
import numpy as np
import matplotlib.pyplot as plt

importances = model.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

#tsne
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

perplexity_value = min(5, X_scaled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_scaled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y.values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='viridis', alpha=0.7)
plt.title('t-SNE visualization')
plt.legend(title='Apnea class')
plt.show()

"""Since we don't have a lot of data, these are the algorithms that I will use:

- Logistic regression
- Support Vector Machine (SVM)
- K-Nearest Neighbour (KNN)
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

model = SVC(kernel='rbf')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Confussion matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generar la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Visualizar la matriz de confusión usando un heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""###Divided in 2  groups

Random forest
"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn import tree

iahs_file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/variables-psg.csv'
iahs_data = pd.read_csv(iahs_file_path, encoding="utf-8", sep=";")

file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data.csv'
df = pd.read_csv(file_path)

#since we separated the patients in different rows, we need to have the same name as the DNI column in iahs_data
df['DNI_temp'] = df['DNI']
df['DNI'] = df['DNI'].str.extract(r'(^who_\d+)')

#assign classes based on types
def assign_class(IAH):
    if IAH < 15:
        return 'Lower level'
    else:
        return 'Higher level'

iahs_data['Class'] = iahs_data['Indice de apneas hipopneas por hora (IAH)'].apply(assign_class)

#merge data
merge_data = df.merge(iahs_data[['DNI', 'Class']], on='DNI', how='inner')

#put back the names of the different patients
merge_data['DNI'] = merge_data['DNI'] + '_' + merge_data['DNI_temp'].str.split('_').str[2]

#delete temp column
merge_data.drop(columns=['DNI_temp'], inplace=True)
merge_data

#checking how many patientshave each type of apnea
unique_patients_in_class = merge_data.drop_duplicates(subset=['DNI'])['Class'].value_counts()

#dividing by 4 because there are 4 rows for each patient
print(unique_patients_in_class/4)

#change apnea classes to numeric
class_mapping = {'Lower level': 0, 'Higher level': 1}
merge_data['Class'] = merge_data['Class'].map(class_mapping)

X = merge_data.drop(columns=['Class', 'DNI'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

print(pd.Series(y_pred).value_counts())

#gini impurity
import numpy as np
import matplotlib.pyplot as plt

importances = model.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

#tsne
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

perplexity_value = min(5, X_scaled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_scaled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y.values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='viridis', alpha=0.7)
plt.title('t-SNE visualization')
plt.legend(title='Apnea class')
plt.show()

"""Since we don't have a lot of data, these are the algorithms that I will use:

- Logistic regression
- Support Vector Machine (SVM)
- K-Nearest Neighbour (KNN)
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

model = SVC(kernel='rbf')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Confussion Matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generar la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Visualizar la matriz de confusión usando un heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""# Version 2

## Data cleaning
"""

import pandas as pd
import os

from google.colab import drive
drive.mount('/content/drive')

path_info = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/'
file_info = 'informacion_pacientes_2.csv'
#selecting the columns necessary for the study
columns = ["DNI", "Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]
info = pd.read_csv(path_info + file_info, usecols = columns,encoding="utf-8", sep = ",")

#change the sex column to binary
info["Sexo"] = info["Sexo"].map({"Hombre": 0, "Mujer": 1})

info.head()

path_vari = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/'
file_vari = 'variables-psg_2.csv'
#selecting the columns necessary for the study
columns_2 = ["DNI", "Tiempo total de sueño -TTS- (minutos):"]
vari = pd.read_csv(path_vari + file_vari, usecols = columns_2, encoding="utf-8", sep = ",")

#checking that the patient has had at least 5 hours of sleep
vari = vari[vari["Tiempo total de sueño -TTS- (minutos):"] > 300]

vari.head()

#merging both datasets so they have the same patients
data_combined = pd.merge(info, vari, on="DNI", how="inner")

#taking out the lines where the patient has no individual csv with their own data
no_data = ["who_006", "who_007", "who_008", "who_012", "who_014", "who_016", "who_019", "who_030", "who_031", "who_033", "who_035", "who_036", "who_037"]
data_combined = data_combined[~data_combined["DNI"].isin(no_data)]

data_combined

#check if the patients have more tahn 5 hours of sleep based on the first and last timestamps
from datetime import datetime, timedelta
all_patients = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes'

patients = set(data_combined['DNI'])

valid_patients = []

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check if the patient really has 5 hours of sleep
      first_timestamp = data_patient[" timestamp"].iloc[0]
      last_timestamp = data_patient[" timestamp"].iloc[-1]

      first_timestamp = datetime.strptime(first_timestamp, "%H_%M_%S")
      last_timestamp = datetime.strptime(last_timestamp, "%H_%M_%S")

      #check if sleep starts after midnight
      if last_timestamp < first_timestamp:
        last_timestamp += timedelta(days=1)

      duration = last_timestamp - first_timestamp

      cutoff = first_timestamp + timedelta(minutes=30)
      data_patient['timestamp'] = data_patient[" timestamp"].apply(lambda x: datetime.strptime(x, "%H_%M_%S"))
      data_patient = data_patient[data_patient['timestamp'] >= cutoff]

      if duration.total_seconds() / 3600 >= 5:
        valid_patients.append(patient_nb)

path_005 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_005/'
all_data_005 = os.listdir(path_005)
save_to = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'

who_005 = []
for csvd in all_data_005:
    df = pd.read_csv(path_005 + csvd, encoding="utf-8", sep = ",")
    who_005.append(df)

patient_005 = pd.concat(who_005, ignore_index=True)
output_file = os.path.join(save_to, "WHO_005_data.csv")
patient_005.to_csv(output_file, index=False, encoding="utf-8")

path_027 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_027/'
all_data_027 = os.listdir(path_027)

who_027 = []
for csvd in all_data_027:
    df = pd.read_csv(path_027 + csvd, encoding="utf-8", sep = ",")
    who_027.append(df)

patient_027 = pd.concat(who_027, ignore_index=True)
output_file = os.path.join(save_to, "WHO_027_data.csv")
patient_027.to_csv(output_file, index=False, encoding="utf-8")

path_0103 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_0103/'
all_data_0103 = os.listdir(path_0103)

who_0103 = []
for csvd in all_data_0103:
    df = pd.read_csv(path_0103 + csvd, encoding="utf-8", sep = ",")
    who_0103.append(df)

patient_0103 = pd.concat(who_0103, ignore_index=True)
output_file = os.path.join(save_to, "WHO_0103_data.csv")
patient_0103.to_csv(output_file, index=False, encoding="utf-8")

valid_patients

#creating a csv for each patient with their bmi, age and sex and saving them on specific patient folders
new_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2/'
bas = ["Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]

for index, row in data_combined.iterrows():
  patient_dni = row['DNI'].lower()
  if patient_dni in valid_patients:
    folder_name = f"{new_folder}_{row['DNI']}"

    if not os.path.exists(folder_name):
      os.makedirs(folder_name)

    row_data = pd.DataFrame([row[bas]])
    file_name = f"{folder_name}/{row['DNI']}_BMI_2.csv"
    row_data.to_csv(file_name, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2/'

hr = [" timestamp", " hr"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the hr
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the hr and timestamp
      patient_hr = data_patient[hr]

      #eliminate all values that are 0
      patient_hr = patient_hr[patient_hr[" hr"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_HR_2.csv")
      patient_hr.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2/'

ibi = [" timestamp", " ibi"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the ibi
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the ibi and timestamp
      patient_ibi = data_patient[ibi]

      #eliminate all values that are 0
      patient_ibi = patient_ibi[patient_ibi[" ibi"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_IBI_2.csv")
      patient_ibi.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2/'

temp = [" timestamp", " temp"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the temp
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "T"]

      #look just for the temp and timestamp
      patient_temp = data_patient[temp]

      #eliminate all values that are 0
      patient_temp = patient_temp[patient_temp[" temp"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_TEMP_2.csv")
      patient_temp.to_csv(output_file, index = False)

origin_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/'
output_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2/'

sop2 = [" timestamp", " sop2"]

for patient_file in os.listdir(origin_folder):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(origin_folder, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the sop2
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "S"]

      #look just for the sop2 and timestamp
      patient_sop2 = data_patient[sop2]

      #eliminate all values that are 0
      patient_sop2 = patient_sop2[patient_sop2[" sop2"] != 0]

      #where to save the data
      patient_folder = os.path.join(output_folder, f"_{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_SOP2_2.csv")
      patient_sop2.to_csv(output_file, index = False)

"""## Data preparation

Create a row in the dataset for each patient. Data to have:

- Sex (bianry number, 0 for male, 1 for female)
- BMI
- First derivate (dHR/dt, dPO2/dt): shows change velocity
- Second derivate (HR and SPO2 acceleration): detects sudden changes
- Number of falls in SPO2 (>3% o 4%): shows saturation events
- Total time with SPO2 (<90%): nocturnal hipoxia
- Number of spikes in HR: nocturnal tachycardia
- Median duration of desaturation events
- SPO2 recuperation ratio: how quickly does the patient recover after apnea
- Oxygen Desaturation Index (ODI): Number of SpO₂ drops of at least 3-4% per hour
"""

import os
import pandas as pd
import numpy as np

patinet_files_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2'
output_fold = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa'

all_patients_data = []

columns = ['DNI', 'sex', 'bmi', 'first_derivate_hr', 'first_derivate_sop2',
           'second_derivate_hr', 'second_derivate_sop2', 'sop2_falls',
           'sop2_total_time', 'hr_spikes', 'mean_desaturation_duration',
           'sop2_recuperation', 'oxygen_desaturation_index_ODI']

#derivatives calculation
def calc_derivatives(signal):
    first_derivative = np.diff(signal)
    second_derivative = np.diff(first_derivative)
    return np.mean(np.abs(first_derivative)), np.mean(np.abs(second_derivative))

#sop2 falls calculation
def count_sop2_falls(sop2, threshold=3):
    falls = (np.diff(sop2) <= -threshold).sum()
    return int(falls)

#sop2 < 90
def time_below_threshold(sop2, threshold=90):
    return (sop2 < threshold).sum()

#hr spikes
def count_hr_spikes(hr, spike_threshold=15):
    return (np.abs(np.diff(hr)) >= spike_threshold).sum()

#desaturation events
def desaturation_metrics(sop2, time_interval=1, desat_threshold=3, recov_threshold=2):
    events = []
    in_event = False
    start_idx = 0
    baseline = sop2[0]
    durations = []
    recovery_ratios = []

    for i in range(1, len(sop2)):
        drop = baseline - sop2[i]
        if not in_event and drop >= desat_threshold:
            in_event = True
            start_idx = i
        elif in_event and sop2[i] >= (baseline - recov_threshold):
            duration = (i - start_idx) * time_interval
            durations.append(duration)
            if sop2[start_idx] > 0:
                recovery_ratios.append((sop2[i] - sop2[start_idx]) / sop2[start_idx])
            in_event = False
            baseline = sop2[i]

    mean_duration = np.mean(durations) if durations else 0
    mean_recovery = np.mean(recovery_ratios) if recovery_ratios else 0
    #odi will represent the number of times the desaturations happened
    odi = len(durations)
    return mean_duration, mean_recovery, odi

#loop for every patient
for patient_folder in os.listdir(patinet_files_path):
    folder_path = os.path.join(patinet_files_path, patient_folder)

    if os.path.isdir(folder_path):
        data_array = [None] * len(columns)
        data_array[0] = patient_folder[1:]

        hr_data = None
        sop2_data = None

        for data_file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, data_file)

            if data_file.endswith("BMI_2.csv"):
                bmi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                data_array[1] = bmi.iloc[0]["Sexo"]
                data_array[2] = bmi.iloc[0]["Indice de masa corporal (peso en Kg/talla en metros²)"]

            elif data_file.endswith("HR_2.csv"):
                hr = pd.read_csv(file_path, encoding="utf-8", sep=",")
                hr_data = hr[" hr"].dropna().values

            elif data_file.endswith("SOP2_2.csv"):
                sop2 = pd.read_csv(file_path, encoding="utf-8", sep=",")
                sop2_data = sop2[" sop2"].dropna().values

        if hr_data is not None and sop2_data is not None:
            data_array[3], data_array[5] = calc_derivatives(hr_data)
            data_array[4], data_array[6] = calc_derivatives(sop2_data)
            data_array[7] = count_sop2_falls(sop2_data)
            data_array[8] = time_below_threshold(sop2_data)
            data_array[9] = count_hr_spikes(hr_data)
            dur, rec, odi = desaturation_metrics(sop2_data)
            data_array[10] = dur
            data_array[11] = rec
            data_array[12] = odi

        all_patients_data.append(data_array)

#save results to csv
df = pd.DataFrame(all_patients_data, columns=columns)
df = df.round(2)
output_csv = os.path.join(output_fold, 'classification_data_2.csv')
df.to_csv(output_csv, index=False)

"""##Data augmentation

For augmentation, use SMOTE and SMOTEN
"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn import tree

iahs_file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/variables-psg_2.csv'
iahs_data = pd.read_csv(iahs_file_path, encoding="utf-8", sep=",")

#assign classes based on types
def assign_class(IAH):
    if IAH < 5:
        return 'No apnea'
    elif 5 <= IAH < 15:
        return 'Mild'
    elif 15 <= IAH < 30:
        return 'Moderate'
    else:
        return 'Severe'

iahs_data['Class'] = iahs_data['Indice de apneas hipopneas por hora (IAH)'].apply(assign_class)

#load data from classification_data.csv
file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data_2.csv'
df = pd.read_csv(file_path)

merge_data = df.merge(iahs_data[['DNI', 'Class']], on='DNI', how='inner')
merge_data

unique_patients_in_class = merge_data.drop_duplicates(subset=['DNI'])['Class'].value_counts()
unique_patients_in_class

"""### No augmentation used"""

#change apnea classes to numeric as smote doesn't work with strings
class_mapping = {'No apnea': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3}
merge_data['Class'] = merge_data['Class'].map(class_mapping)

X = merge_data.drop(columns=['Class', 'DNI'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

#gini impurity
import numpy as np
import matplotlib.pyplot as plt

importances = model.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

#tsne
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

perplexity_value = min(5, X_scaled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_scaled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y.values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='viridis', alpha=0.7)
plt.title('t-SNE visualization')
plt.legend(title='Apnea class')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

model = SVC(kernel='rbf')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generar la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Visualizar la matriz de confusión usando un heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""### SMOTE used"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

# Separar features y labels
X = merge_data.drop(columns=['DNI', 'Class'])  # Solo las features numéricas
y = merge_data['Class']  # Ya están codificadas con números

# Escalado
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Volver a transformar las clases numéricas a nombres
inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

# Mostrar la distribución después de SMOTE
print("Distribución después de SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

#gini impurity
import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# t-SNE con los datos aumentados (X_resampled ya está escalado)
perplexity_value = min(30, X_resampled.shape[0] - 1)  # más grande porque hay más datos

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

# Crear DataFrame para graficar
tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels  # Nombres legibles

# Visualización
plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression()
model.fit(X_train_re, y_train_re)
y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.svm import SVC

model = SVC(kernel='rbf')
model.fit(X_train_re, y_train_re)

y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train_re, y_train_re)

y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generar la matriz de confusión
cm = confusion_matrix(y_test_re, y_pred_re)

# Visualizar la matriz de confusión usando un heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""###SMOTE used (20 cases for each class)"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

# Separar features y labels
X = merge_data.drop(columns=['DNI', 'Class'])  # Solo las features numéricas
y = merge_data['Class']  # Ya están codificadas con números

# Escalado
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Definir cuántos ejemplos queremos por clase
desired_samples = 20

# Creamos un diccionario con las clases y cuántas muestras queremos que tengan
sampling_strategy = {
    0: desired_samples,  # No apnea
    1: desired_samples,  # Mild
    2: desired_samples,  # Moderate
    3: desired_samples   # Severe
}

# Aplicar SMOTE
smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=2)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Volver a transformar las clases numéricas a nombres
inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

# Mostrar la distribución después de SMOTE
print("Distribución después de SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

#gini impurity
import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# t-SNE con los datos aumentados (X_resampled ya está escalado)
perplexity_value = min(30, X_resampled.shape[0] - 1)  # más grande porque hay más datos

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

# Crear DataFrame para graficar
tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels  # Nombres legibles

# Visualización
plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression()
model.fit(X_train_re, y_train_re)
y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.svm import SVC

model = SVC(kernel='rbf')
model.fit(X_train_re, y_train_re)

y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train_re, y_train_re)

y_pred_re = model.predict(X_test_re)

print(accuracy_score(y_test_re, y_pred_re))
print(classification_report(y_test_re, y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generar la matriz de confusión
cm = confusion_matrix(y_test_re, y_pred_re)

# Visualizar la matriz de confusión usando un heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Confusion Matrix')
plt.show()

"""##Separation in columns"""

import os
import pandas as pd
import math

patinet_files_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 2'
output_fold = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa'

all_patients_data = []

columns = ['DNI', 'sex', 'bmi', 'first_derivate_hr_q1', 'first_derivate_sop2_q1', 'second_derivate_hr_q1', 'second_derivate_sop2_q1', 'sop2_falls_q1',
           'sop2_total_time_q1', 'hr_spikes_q1', 'mean_desaturation_duration_q1', 'sop2_recuperation_q1', 'oxygen_desaturation_index_ODI_q1',
           'first_derivate_hr_q2', 'first_derivate_sop2_q2', 'second_derivate_hr_q2', 'second_derivate_sop2_q2', 'sop2_falls_q2',
           'sop2_total_time_q2', 'hr_spikes_q2', 'mean_desaturation_duration_q2', 'sop2_recuperation_q2', 'oxygen_desaturation_index_ODI_q2',
           'first_derivate_hr_q3', 'first_derivate_sop2_q3', 'second_derivate_hr_q3', 'second_derivate_sop2_q3', 'sop2_falls_q3',
           'sop2_total_time_q3', 'hr_spikes_q3', 'mean_desaturation_duration_q3', 'sop2_recuperation_q3', 'oxygen_desaturation_index_ODI_q3',
           'first_derivate_hr_q4', 'first_derivate_sop2_q4', 'second_derivate_hr_q4', 'second_derivate_sop2_q4', 'sop2_falls_q4',
           'sop2_total_time_q4', 'hr_spikes_q4', 'mean_desaturation_duration_q4', 'sop2_recuperation_q4', 'oxygen_desaturation_index_ODI_q4']

def calc_derivatives(signal):
    first_derivative = np.diff(signal)
    second_derivative = np.diff(first_derivative)
    return np.mean(np.abs(first_derivative)), np.mean(np.abs(second_derivative))

def count_sop2_falls(sop2, threshold=3):
    return int((np.diff(sop2) <= -threshold).sum())

def time_below_threshold(sop2, threshold=90):
    return (sop2 < threshold).sum()

def count_hr_spikes(hr, spike_threshold=15):
    return (np.abs(np.diff(hr)) >= spike_threshold).sum()

def desaturation_metrics(sop2, time_interval=1, desat_threshold=3, recov_threshold=2):
    durations = []
    recovery_ratios = []
    in_event = False
    start_idx = 0
    baseline = sop2[0]

    for i in range(1, len(sop2)):
        drop = baseline - sop2[i]
        if not in_event and drop >= desat_threshold:
            in_event = True
            start_idx = i
        elif in_event and sop2[i] >= (baseline - recov_threshold):
            duration = (i - start_idx) * time_interval
            durations.append(duration)
            if sop2[start_idx] > 0:
                recovery_ratios.append((sop2[i] - sop2[start_idx]) / sop2[start_idx])
            in_event = False
            baseline = sop2[i]

    mean_duration = np.mean(durations) if durations else 0
    mean_recovery = np.mean(recovery_ratios) if recovery_ratios else 0
    odi = len(durations)
    return mean_duration, mean_recovery, odi

#round rumbers when dividing the number of lines
def safe_round(value):
    if pd.isna(value):
        return 0  #if the number is NA, change it to 0
    return round(value)

#function to divide in 4 groups
def divide_in_groups(df, group_size):
    #round to lower value
    n_groups = math.floor(len(df) / group_size)
    groups = []

    #create new groups
    for i in range(n_groups):
        start = i * group_size
        end = start + group_size
        group = df.iloc[start:end]
        groups.append(group)

    #add rows if they are not divisible by 4
    if len(df) % group_size != 0:
        group = df.iloc[n_groups * group_size:]
        groups.append(group)

    return groups

#go throught each patient
for patient_folder in os.listdir(patinet_files_path):
    folder_path = os.path.join(patinet_files_path, patient_folder)

    if os.path.isdir(folder_path):
        #get rid of '_' at the start of the folder name temporally to check the names properly
        patient_id = patient_folder[1:]

        data_array = [None] * len(columns)
        data_array[0] = patient_id

        for data_file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, data_file)

            if data_file.endswith("BMI_2.csv"):
                bmi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                data_array[0] = patient_id
                data_array[1] = bmi.iloc[0]["Sexo"]
                data_array[2] = safe_round(bmi.iloc[0]["Indice de masa corporal (peso en Kg/talla en metros²)"])

            elif data_file.endswith("HR_2.csv"):
                hr = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(hr) / 4)
                hr_groups = divide_in_groups(hr, group_size)

                for i, group in enumerate(hr_groups, 1):
                  if i==1:
                    data_array[3] = calc_derivatives(hr[" hr"])[0]
                    data_array[5] = calc_derivatives(hr[" hr"])[1]
                    data_array[9] = count_hr_spikes(hr[" hr"])
                  elif i==2:
                    data_array[13] = calc_derivatives(hr[" hr"])[0]
                    data_array[15] = calc_derivatives(hr[" hr"])[1]
                    data_array[19] = count_hr_spikes(hr[" hr"])
                  elif i==3:
                    data_array[23] = calc_derivatives(hr[" hr"])[0]
                    data_array[25] = calc_derivatives(hr[" hr"])[1]
                    data_array[29] = count_hr_spikes(hr[" hr"])
                  elif i==4:
                    data_array[33] = calc_derivatives(hr[" hr"])[0]
                    data_array[35] = calc_derivatives(hr[" hr"])[1]
                    data_array[39] = count_hr_spikes(hr[" hr"])

            elif data_file.endswith("SOP2_2.csv"):
                sop2 = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(sop2) / 4)
                sop2_groups = divide_in_groups(sop2, group_size)

                for i, group in enumerate(sop2_groups, 1):
                    if i==1:
                      data_array[4] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[6] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[7] = count_sop2_falls(sop2[" sop2"])
                      data_array[8] = time_below_threshold(sop2[" sop2"])
                      data_array[10] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[11] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[12] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==2:
                      data_array[14] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[16] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[17] = count_sop2_falls(sop2[" sop2"])
                      data_array[18] = time_below_threshold(sop2[" sop2"])
                      data_array[20] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[21] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[22] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==3:
                      data_array[24] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[26] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[27] = count_sop2_falls(sop2[" sop2"])
                      data_array[28] = time_below_threshold(sop2[" sop2"])
                      data_array[30] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[31] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[32] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==4:
                      data_array[34] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[36] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[37] = count_sop2_falls(sop2[" sop2"])
                      data_array[38] = time_below_threshold(sop2[" sop2"])
                      data_array[40] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[41] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[42] = desaturation_metrics(sop2[" sop2"])[2]

        all_patients_data.append(data_array)
#save data
df = pd.DataFrame(all_patients_data, columns=columns)
output_csv = os.path.join(output_fold, 'classification_data_separation.csv')
df.to_csv(output_csv, index=False)
df.head(10)

import pandas as pd

sep_df = pd.read_csv('/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data_separation.csv')

X = sep_df.drop(columns=['Class', 'DNI'])
y = sep_df['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

"""##CNN

Capas a usar:

- Conv2D: detectar relaciones
- MaxPooling2D: reduce y enfoca en patrones importantes
- Flatten: datos a vector para clasificación
- Dense: clasificación (no apnea, mild, moderate, severe)
- Dropout: regularización
- BatchNormalization: estabilizar aprendizaje

# Version 3

## Data cleaning
"""

import pandas as pd
import os

from google.colab import drive
drive.mount('/content/drive')

main_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/'
patient_info = 'informacion_pacientes_3.csv'
#selecting the columns necessary for the study
columns = ["DNI", "Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]
pat_info = pd.read_csv(main_path + patient_info, usecols = columns,encoding="utf-8", sep = ",")

#change the sex column to binary
pat_info["Sexo"] = pat_info["Sexo"].map({"Hombre": 0, "Mujer": 1})

pat_info.head()

file_vari = 'variables-psg_3.csv'
#selecting the columns necessary for the study
columns_2 = ["DNI", "Tiempo total de sueño -TTS- (minutos):"]
vari = pd.read_csv(main_path + file_vari, usecols = columns_2, encoding="utf-8", sep = ",")

#checking that the patient has had at least 5 hours of sleep
vari = vari[vari["Tiempo total de sueño -TTS- (minutos):"] > 300]

vari.head()

#merging both datasets so they have the same patients
data_combined = pd.merge(pat_info, vari, on="DNI", how="inner")

#taking out the lines where the patient has no individual csv with their own data
no_data = ["who_006", "who_007", "who_008", "who_012", "who_014", "who_016", "who_019", "who_030", "who_031", "who_033", "who_035", "who_036", "who_037"]
data_combined = data_combined[~data_combined["DNI"].isin(no_data)]

data_combined.head()

#check if the patients have more tahn 5 hours of sleep based on the first and last timestamps
from datetime import datetime, timedelta
all_patients = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes'

patients = set(data_combined['DNI'])

valid_patients = []

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check if the patient really has 5 hours of sleep
      first_timestamp = data_patient[" timestamp"].iloc[0]
      last_timestamp = data_patient[" timestamp"].iloc[-1]

      first_timestamp = datetime.strptime(first_timestamp, "%H_%M_%S")
      last_timestamp = datetime.strptime(last_timestamp, "%H_%M_%S")

      #check if sleep starts after midnight
      if last_timestamp < first_timestamp:
        last_timestamp += timedelta(days=1)

      duration = last_timestamp - first_timestamp

      cutoff = first_timestamp + timedelta(minutes=30)
      data_patient['timestamp'] = data_patient[" timestamp"].apply(lambda x: datetime.strptime(x, "%H_%M_%S"))
      data_patient = data_patient[data_patient['timestamp'] >= cutoff]

      if duration.total_seconds() / 3600 >= 5:
        valid_patients.append(patient_nb)

path_005 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_005/'
all_data_005 = os.listdir(path_005)

who_005 = []
for csvd in all_data_005:
    df = pd.read_csv(path_005 + csvd, encoding="utf-8", sep = ",")
    who_005.append(df)

patient_005 = pd.concat(who_005, ignore_index=True)
output_file = os.path.join(all_patients, "WHO_005_data.csv")
patient_005.to_csv(output_file, index=False, encoding="utf-8")

path_027 = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datasets pacientes/WHO_027/'
all_data_027 = os.listdir(path_027)

who_027 = []
for csvd in all_data_027:
    df = pd.read_csv(path_027 + csvd, encoding="utf-8", sep = ",")
    who_027.append(df)

patient_027 = pd.concat(who_027, ignore_index=True)
output_file = os.path.join(all_patients, "WHO_027_data.csv")
patient_027.to_csv(output_file, index=False, encoding="utf-8")

valid_patients

#creating a csv for each patient with their bmi, age and sex and saving them on specific patient folders
new_folder = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 3/'
bas = ["Edad", "Sexo", "Indice de masa corporal (peso en Kg/talla en metros²)"]

for index, row in data_combined.iterrows():
  patient_dni = row['DNI'].lower()
  if patient_dni in valid_patients:
    folder_name = f"{new_folder}{row['DNI']}"

    if not os.path.exists(folder_name):
      os.makedirs(folder_name)

    row_data = pd.DataFrame([row[bas]])
    file_name = f"{folder_name}/{row['DNI']}_BMI.csv"
    row_data.to_csv(file_name, index = False)

hr = [" timestamp", " hr"]

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the hr
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the hr and timestamp
      patient_hr = data_patient[hr]

      #eliminate all values that are 0
      patient_hr = patient_hr[patient_hr[" hr"] != 0]

      #where to save the data
      patient_folder = os.path.join(new_folder, f"{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_HR.csv")
      patient_hr.to_csv(output_file, index = False)

ibi = [" timestamp", " ibi"]

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the ibi
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "H"]

      #look just for the ibi and timestamp
      patient_ibi = data_patient[ibi]

      #eliminate all values that are 0
      patient_ibi = patient_ibi[patient_ibi[" ibi"] != 0]

      #where to save the data
      patient_folder = os.path.join(new_folder, f"{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_IBI.csv")
      patient_ibi.to_csv(output_file, index = False)

temp = [" timestamp", " temp"]

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the temp
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "T"]

      #look just for the temp and timestamp
      patient_temp = data_patient[temp]

      #eliminate all values that are 0
      patient_temp = patient_temp[patient_temp[" temp"] != 0]

      #where to save the data
      patient_folder = os.path.join(new_folder, f"{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_TEMP.csv")
      patient_temp.to_csv(output_file, index = False)

sop2 = [" timestamp", " sop2"]

for patient_file in os.listdir(all_patients):
  if patient_file.endswith(".csv"): #since there are folders on the folder itself, look just for the csv
    patient_nb = patient_file.lower()[:-9] #make them have the same format for comparison

    if patient_nb in valid_patients:
      #look for patient data on folder
      file_path = os.path.join(all_patients, patient_file)
      data_patient = pd.read_csv(file_path, encoding="utf-8", sep = ",")

      #check that the sensor got the sop2
      data_patient = data_patient[data_patient[" sensor "].str.strip().str.upper() == "S"]

      #look just for the sop2 and timestamp
      patient_sop2 = data_patient[sop2]

      #eliminate all values that are 0
      patient_sop2 = patient_sop2[patient_sop2[" sop2"] != 0]

      #where to save the data
      patient_folder = os.path.join(new_folder, f"{patient_nb}")

      #create folder if it doenst exists already
      if not os.path.exists(patient_folder):
        os.makedirs(patient_folder)

      #save the csv
      output_file = os.path.join(patient_folder, f"{patient_nb}_SOP2.csv")
      patient_sop2.to_csv(output_file, index = False)

"""## Data preparation

Create a row in the dataset for each patient. Data to have:

- Sex (bianry number, 0 for male, 1 for female)
- BMI
- First derivate (dHR/dt, dPO2/dt): shows change velocity
- Second derivate (HR and SPO2 acceleration): detects sudden changes
- Number of falls in SPO2 (>3% o 4%): shows saturation events
- Total time with SPO2 (<90%): nocturnal hipoxia
- Number of spikes in HR: nocturnal tachycardia
- Median duration of desaturation events
- SPO2 recuperation ratio: how quickly does the patient recover after apnea
- Oxygen Desaturation Index (ODI): Number of SpO₂ drops of at least 3-4% per hour
"""

import os
import pandas as pd
import numpy as np

all_patients_data = []

columns = ['DNI', 'sex', 'bmi', 'first_derivate_hr', 'first_derivate_sop2',
           'second_derivate_hr', 'second_derivate_sop2', 'sop2_falls',
           'sop2_total_time', 'hr_spikes', 'mean_desaturation_duration',
           'sop2_recuperation', 'oxygen_desaturation_index_ODI']

#derivatives calculation
def calc_derivatives(signal):
    first_derivative = np.diff(signal)
    second_derivative = np.diff(first_derivative)
    return np.mean(np.abs(first_derivative)), np.mean(np.abs(second_derivative))

#sop2 falls calculation
def count_sop2_falls(sop2, threshold=3):
    falls = (np.diff(sop2) <= -threshold).sum()
    return int(falls)

#sop2 < 90
def time_below_threshold(sop2, threshold=90):
    return (sop2 < threshold).sum()

#hr spikes
def count_hr_spikes(hr, spike_threshold=15):
    return (np.abs(np.diff(hr)) >= spike_threshold).sum()

#desaturation events
def desaturation_metrics(sop2, time_interval=1, desat_threshold=3, recov_threshold=2):
    events = []
    in_event = False
    start_idx = 0
    baseline = sop2[0]
    durations = []
    recovery_ratios = []

    for i in range(1, len(sop2)):
        drop = baseline - sop2[i]
        if not in_event and drop >= desat_threshold:
            in_event = True
            start_idx = i
        elif in_event and sop2[i] >= (baseline - recov_threshold):
            duration = (i - start_idx) * time_interval
            durations.append(duration)
            if sop2[start_idx] > 0:
                recovery_ratios.append((sop2[i] - sop2[start_idx]) / sop2[start_idx])
            in_event = False
            baseline = sop2[i]

    mean_duration = np.mean(durations) if durations else 0
    mean_recovery = np.mean(recovery_ratios) if recovery_ratios else 0
    #odi will represent the number of times the desaturations happened
    odi = len(durations)
    return mean_duration, mean_recovery, odi

#loop for every patient
for patient_folder in os.listdir(new_folder):
    folder_path = os.path.join(new_folder, patient_folder)

    if os.path.isdir(folder_path):
        data_array = [None] * len(columns)
        data_array[0] = patient_folder

        hr_data = None
        sop2_data = None

        for data_file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, data_file)

            if data_file.endswith("BMI.csv"):
                bmi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                data_array[1] = bmi.iloc[0]["Sexo"]
                data_array[2] = bmi.iloc[0]["Indice de masa corporal (peso en Kg/talla en metros²)"]

            elif data_file.endswith("HR.csv"):
                hr = pd.read_csv(file_path, encoding="utf-8", sep=",")
                hr_data = hr[" hr"].dropna().values

            elif data_file.endswith("SOP2.csv"):
                sop2 = pd.read_csv(file_path, encoding="utf-8", sep=",")
                sop2_data = sop2[" sop2"].dropna().values

        if hr_data is not None and sop2_data is not None:
            data_array[3], data_array[5] = calc_derivatives(hr_data)
            data_array[4], data_array[6] = calc_derivatives(sop2_data)
            data_array[7] = count_sop2_falls(sop2_data)
            data_array[8] = time_below_threshold(sop2_data)
            data_array[9] = count_hr_spikes(hr_data)
            dur, rec, odi = desaturation_metrics(sop2_data)
            data_array[10] = dur
            data_array[11] = rec
            data_array[12] = odi

        all_patients_data.append(data_array)

#save results to csv
df = pd.DataFrame(all_patients_data, columns=columns)
df = df.round(2)
output_csv = os.path.join(main_path, 'classification_data_3.csv')
df.to_csv(output_csv, index=False)

"""## Data augmentation"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn import tree

iahs_file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/variables-psg_3.csv'
iahs_data = pd.read_csv(iahs_file_path, encoding="utf-8", sep=",")

#assign classes based on types
def assign_class(IAH):
    if IAH < 5:
        return 'No apnea'
    elif 5 <= IAH < 15:
        return 'Mild'
    elif 15 <= IAH < 30:
        return 'Moderate'
    else:
        return 'Severe'

iahs_data['Class'] = iahs_data['Indice de apneas hipopneas por hora (IAH)'].apply(assign_class)

#load data from classification_data.csv
file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data_3.csv'
df = pd.read_csv(file_path)

merge_data = df.merge(iahs_data[['DNI', 'Class']], on='DNI', how='inner')
merge_data

unique_patients_in_class = merge_data.drop_duplicates(subset=['DNI'])['Class'].value_counts()
unique_patients_in_class

"""### Data classification with no augmentation"""

#change apnea classes to numeric as smote doesn't work with strings
class_mapping = {'No apnea': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3}
merge_data['Class'] = merge_data['Class'].map(class_mapping)

X = merge_data.drop(columns=['Class', 'DNI'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

import numpy as np
import matplotlib.pyplot as plt

importances = model.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

perplexity_value = min(5, X_scaled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_scaled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y.values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='viridis', alpha=0.7)
plt.title('t-SNE visualization')
plt.legend(title='Apnea class')
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)

print(accuracy_score(y_test, lr_y_pred))
print(classification_report(y_test, lr_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test, lr_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Logistic Regression Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train, y_train)

svc_y_pred = svc_model.predict(X_test)

print(accuracy_score(y_test, svc_y_pred))
print(classification_report(y_test, svc_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test, svc_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('SVC Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train, y_train)

k_y_pred = k_model.predict(X_test)

print(accuracy_score(y_test, k_y_pred))
print(classification_report(y_test, k_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test, k_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""#### CNN"""

nFeats = 12
nClasses = 4
time_frames = 128 #for these models, this variable won't be used, as the data not sequential

X_train_cnn = X_train.reshape(-1, nFeats, 1, 1)
X_test_cnn = X_test.reshape(-1, nFeats, 1, 1)

y_train = y_train.astype(np.int32)
y_test = y_test.astype(np.int32)

import tensorflow as tf

def build_cnn_from_script(input_shape, n_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(8, (4, nFeats), padding="same", activation="relu", input_shape=input_shape),
        tf.keras.layers.MaxPool2D(pool_size=(2, 1)),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Conv2D(16, (4, 1), padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((3, 1), padding="same"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(16, activation="relu"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(n_classes, activation="softmax")
    ])
    return model

model = build_cnn_from_script(input_shape=(nFeats, 1, 1), n_classes=nClasses)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_cnn, y_train, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

loss, acc = model.evaluate(X_test_cnn, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = np.argmax(model.predict(X_test_cnn), axis=1)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""### Data classification with SMOTE"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

X = merge_data.drop(columns=['DNI', 'Class'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

smote = SMOTE(random_state=42, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

print("Class distribution after using SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test_re, y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

perplexity_value = min(30, X_resampled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train_re, y_train_re)
lr_y_pred_re = lr_model.predict(X_test_re)

print(accuracy_score(y_test_re, lr_y_pred_re))
print(classification_report(y_test_re, lr_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test_re, lr_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train_re, y_train_re)

svc_y_pred_re = svc_model.predict(X_test_re)

print(accuracy_score(y_test_re, svc_y_pred_re))
print(classification_report(y_test_re, svc_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test_re, svc_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train_re, y_train_re)

k_y_pred_re = k_model.predict(X_test_re)

print(accuracy_score(y_test_re, k_y_pred_re))
print(classification_report(y_test_re, k_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test_re, k_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""#### CNN"""

nFeats = 12
nClasses = 4
time_frames = 128 #for these models, this variable won't be used, as the data not sequential

from sklearn.preprocessing import LabelEncoder

X_train_cnn = X_train_re.reshape(-1, nFeats, 1, 1)
X_test_cnn = X_test_re.reshape(-1, nFeats, 1, 1)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train_re)
y_test_encoded = label_encoder.transform(y_test_re)

y_train_encoded = np.array(y_train_encoded).astype(np.int32)
y_test_encoded = np.array(y_test_encoded).astype(np.int32)

import tensorflow as tf

def build_cnn_from_script(input_shape, n_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(8, (4, nFeats), padding="same", activation="relu", input_shape=input_shape),
        tf.keras.layers.MaxPool2D(pool_size=(2, 1)),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Conv2D(16, (4, 1), padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((3, 1), padding="same"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(16, activation="relu"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(n_classes, activation="softmax")
    ])
    return model

model = build_cnn_from_script(input_shape=(nFeats, 1, 1), n_classes=nClasses)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_cnn, y_train_encoded, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

loss, acc = model.evaluate(X_test_cnn, y_test_encoded)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = np.argmax(model.predict(X_test_cnn), axis=1)
print("Confusion Matrix:\n", confusion_matrix(y_test_encoded, y_pred))
print("Classification Report:\n", classification_report(y_test_encoded, y_pred))

"""### Data classification with SMOTE (creating 20 patients for each class)"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

X = merge_data.drop(columns=['DNI', 'Class'])
y = merge_data['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

desired_samples = 20

sampling_strategy = {
    0: desired_samples,
    1: desired_samples,
    2: desired_samples,
    3: desired_samples
}

smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

print("Class distribution after using SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test_re, y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

perplexity_value = min(30, X_resampled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train_re, y_train_re)
lr_y_pred_re = lr_model.predict(X_test_re)

print(accuracy_score(y_test_re, lr_y_pred_re))
print(classification_report(y_test_re, lr_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test_re, lr_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train_re, y_train_re)

svc_y_pred_re = svc_model.predict(X_test_re)

print(accuracy_score(y_test_re, svc_y_pred_re))
print(classification_report(y_test_re, svc_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test_re, svc_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train_re, y_train_re)

k_y_pred_re = k_model.predict(X_test_re)

print(accuracy_score(y_test_re, k_y_pred_re))
print(classification_report(y_test_re, k_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test_re, k_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""#### CNN"""

nFeats = 12
nClasses = 4
time_frames = 128 #for these models, this variable won't be used, as the data not sequential

from sklearn.preprocessing import LabelEncoder

X_train_cnn = X_train_re.reshape(-1, nFeats, 1, 1)
X_test_cnn = X_test_re.reshape(-1, nFeats, 1, 1)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train_re)
y_test_encoded = label_encoder.transform(y_test_re)

y_train_encoded = np.array(y_train_encoded).astype(np.int32)
y_test_encoded = np.array(y_test_encoded).astype(np.int32)

import tensorflow as tf

def build_cnn_from_script(input_shape, n_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(8, (4, nFeats), padding="same", activation="relu", input_shape=input_shape),
        tf.keras.layers.MaxPool2D(pool_size=(2, 1)),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Conv2D(16, (4, 1), padding="same", activation="relu"),
        tf.keras.layers.MaxPool2D((3, 1), padding="same"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(16, activation="relu"),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(n_classes, activation="softmax")
    ])
    return model

model = build_cnn_from_script(input_shape=(nFeats, 1, 1), n_classes=nClasses)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train_cnn, y_train_encoded, validation_split=0.2, epochs=30, batch_size=32, verbose=1)

loss, acc = model.evaluate(X_test_cnn, y_test_encoded)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {acc:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = np.argmax(model.predict(X_test_cnn), axis=1)
print("Confusion Matrix:\n", confusion_matrix(y_test_encoded, y_pred))
print("Classification Report:\n", classification_report(y_test_encoded, y_pred))

"""## Separate data in columns to get more features"""

import os
import pandas as pd
import math

patinet_files_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/Datos filtrados 3'
output_fold = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa'

all_patients_data = []

columns = ['DNI', 'sex', 'bmi', 'first_derivate_hr_q1', 'first_derivate_sop2_q1', 'second_derivate_hr_q1', 'second_derivate_sop2_q1', 'sop2_falls_q1',
           'sop2_total_time_q1', 'hr_spikes_q1', 'mean_desaturation_duration_q1', 'sop2_recuperation_q1', 'oxygen_desaturation_index_ODI_q1',
           'first_derivate_hr_q2', 'first_derivate_sop2_q2', 'second_derivate_hr_q2', 'second_derivate_sop2_q2', 'sop2_falls_q2',
           'sop2_total_time_q2', 'hr_spikes_q2', 'mean_desaturation_duration_q2', 'sop2_recuperation_q2', 'oxygen_desaturation_index_ODI_q2',
           'first_derivate_hr_q3', 'first_derivate_sop2_q3', 'second_derivate_hr_q3', 'second_derivate_sop2_q3', 'sop2_falls_q3',
           'sop2_total_time_q3', 'hr_spikes_q3', 'mean_desaturation_duration_q3', 'sop2_recuperation_q3', 'oxygen_desaturation_index_ODI_q3',
           'first_derivate_hr_q4', 'first_derivate_sop2_q4', 'second_derivate_hr_q4', 'second_derivate_sop2_q4', 'sop2_falls_q4',
           'sop2_total_time_q4', 'hr_spikes_q4', 'mean_desaturation_duration_q4', 'sop2_recuperation_q4', 'oxygen_desaturation_index_ODI_q4']

def calc_derivatives(signal):
    first_derivative = np.diff(signal)
    second_derivative = np.diff(first_derivative)
    return np.mean(np.abs(first_derivative)), np.mean(np.abs(second_derivative))

def count_sop2_falls(sop2, threshold=3):
    return int((np.diff(sop2) <= -threshold).sum())

def time_below_threshold(sop2, threshold=90):
    return (sop2 < threshold).sum()

def count_hr_spikes(hr, spike_threshold=15):
    return (np.abs(np.diff(hr)) >= spike_threshold).sum()

def desaturation_metrics(sop2, time_interval=1, desat_threshold=3, recov_threshold=2):
    durations = []
    recovery_ratios = []
    in_event = False
    start_idx = 0
    baseline = sop2[0]

    for i in range(1, len(sop2)):
        drop = baseline - sop2[i]
        if not in_event and drop >= desat_threshold:
            in_event = True
            start_idx = i
        elif in_event and sop2[i] >= (baseline - recov_threshold):
            duration = (i - start_idx) * time_interval
            durations.append(duration)
            if sop2[start_idx] > 0:
                recovery_ratios.append((sop2[i] - sop2[start_idx]) / sop2[start_idx])
            in_event = False
            baseline = sop2[i]

    mean_duration = np.mean(durations) if durations else 0
    mean_recovery = np.mean(recovery_ratios) if recovery_ratios else 0
    odi = len(durations)
    return mean_duration, mean_recovery, odi

#round rumbers when dividing the number of lines
def safe_round(value):
    if pd.isna(value):
        return 0  #if the number is NA, change it to 0
    return round(value)

#function to divide in 4 groups
def divide_in_groups(df, group_size):
    #round to lower value
    n_groups = math.floor(len(df) / group_size)
    groups = []

    #create new groups
    for i in range(n_groups):
        start = i * group_size
        end = start + group_size
        group = df.iloc[start:end]
        groups.append(group)

    #add rows if they are not divisible by 4
    if len(df) % group_size != 0:
        group = df.iloc[n_groups * group_size:]
        groups.append(group)

    return groups

#go throught each patient
for patient_folder in os.listdir(patinet_files_path):
    folder_path = os.path.join(patinet_files_path, patient_folder)

    if os.path.isdir(folder_path):
        patient_id = patient_folder

        data_array = [None] * len(columns)
        data_array[0] = patient_id

        for data_file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, data_file)

            if data_file.endswith("BMI.csv"):
                bmi = pd.read_csv(file_path, encoding="utf-8", sep=",")
                data_array[0] = patient_id
                data_array[1] = bmi.iloc[0]["Sexo"]
                data_array[2] = safe_round(bmi.iloc[0]["Indice de masa corporal (peso en Kg/talla en metros²)"])

            elif data_file.endswith("HR.csv"):
                hr = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(hr) / 4)
                hr_groups = divide_in_groups(hr, group_size)

                for i, group in enumerate(hr_groups, 1):
                  if i==1:
                    data_array[3] = calc_derivatives(hr[" hr"])[0]
                    data_array[5] = calc_derivatives(hr[" hr"])[1]
                    data_array[9] = count_hr_spikes(hr[" hr"])
                  elif i==2:
                    data_array[13] = calc_derivatives(hr[" hr"])[0]
                    data_array[15] = calc_derivatives(hr[" hr"])[1]
                    data_array[19] = count_hr_spikes(hr[" hr"])
                  elif i==3:
                    data_array[23] = calc_derivatives(hr[" hr"])[0]
                    data_array[25] = calc_derivatives(hr[" hr"])[1]
                    data_array[29] = count_hr_spikes(hr[" hr"])
                  elif i==4:
                    data_array[33] = calc_derivatives(hr[" hr"])[0]
                    data_array[35] = calc_derivatives(hr[" hr"])[1]
                    data_array[39] = count_hr_spikes(hr[" hr"])

            elif data_file.endswith("SOP2.csv"):
                sop2 = pd.read_csv(file_path, encoding="utf-8", sep=",")
                group_size = math.floor(len(sop2) / 4)
                sop2_groups = divide_in_groups(sop2, group_size)

                for i, group in enumerate(sop2_groups, 1):
                    if i==1:
                      data_array[4] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[6] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[7] = count_sop2_falls(sop2[" sop2"])
                      data_array[8] = time_below_threshold(sop2[" sop2"])
                      data_array[10] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[11] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[12] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==2:
                      data_array[14] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[16] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[17] = count_sop2_falls(sop2[" sop2"])
                      data_array[18] = time_below_threshold(sop2[" sop2"])
                      data_array[20] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[21] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[22] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==3:
                      data_array[24] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[26] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[27] = count_sop2_falls(sop2[" sop2"])
                      data_array[28] = time_below_threshold(sop2[" sop2"])
                      data_array[30] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[31] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[32] = desaturation_metrics(sop2[" sop2"])[2]
                    elif i==4:
                      data_array[34] = calc_derivatives(sop2[" sop2"])[0]
                      data_array[36] = calc_derivatives(sop2[" sop2"])[1]
                      data_array[37] = count_sop2_falls(sop2[" sop2"])
                      data_array[38] = time_below_threshold(sop2[" sop2"])
                      data_array[40] = desaturation_metrics(sop2[" sop2"])[0]
                      data_array[41] = desaturation_metrics(sop2[" sop2"])[1]
                      data_array[42] = desaturation_metrics(sop2[" sop2"])[2]

        all_patients_data.append(data_array)
#save data
df = pd.DataFrame(all_patients_data, columns=columns)
output_csv = os.path.join(output_fold, 'classification_data_separation_3.csv')
df.to_csv(output_csv, index=False)
df.head(10)

"""## Data augmentation with separated data"""

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn import tree

iahs_file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/variables-psg_3.csv'
iahs_data = pd.read_csv(iahs_file_path, encoding="utf-8", sep=",")

#assign classes based on types
def assign_class(IAH):
    if IAH < 5:
        return 'No apnea'
    elif 5 <= IAH < 15:
        return 'Mild'
    elif 15 <= IAH < 30:
        return 'Moderate'
    else:
        return 'Severe'

iahs_data['Class'] = iahs_data['Indice de apneas hipopneas por hora (IAH)'].apply(assign_class)

#load data from classification_data.csv
file_path = '/content/drive/My Drive/Ainhoa_WHO/Trabajo Ainhoa/classification_data_separation_3.csv'
df = pd.read_csv(file_path)

merge_data_sep = df.merge(iahs_data[['DNI', 'Class']], on='DNI', how='inner')
merge_data_sep

unique_patients_in_class = merge_data_sep.drop_duplicates(subset=['DNI'])['Class'].value_counts()
unique_patients_in_class

"""### Data classification with no augmentation"""

#change apnea classes to numeric as smote doesn't work with strings
class_mapping = {'No apnea': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3}
merge_data_sep['Class'] = merge_data_sep['Class'].map(class_mapping)

X = merge_data_sep.drop(columns=['Class', 'DNI'])
y = merge_data_sep['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

importances = model.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

perplexity_value = min(5, X_scaled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_scaled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y.values

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='viridis', alpha=0.7)
plt.title('t-SNE visualization')
plt.legend(title='Apnea class')
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_y_pred = lr_model.predict(X_test)

print(accuracy_score(y_test, lr_y_pred))
print(classification_report(y_test, lr_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test, lr_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Logistic Regression Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train, y_train)

svc_y_pred = svc_model.predict(X_test)

print(accuracy_score(y_test, svc_y_pred))
print(classification_report(y_test, svc_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test, svc_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('SVC Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train, y_train)

k_y_pred = k_model.predict(X_test)

print(accuracy_score(y_test, k_y_pred))
print(classification_report(y_test, k_y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test, k_y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""### Data classification with SMOTE"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

X = merge_data_sep.drop(columns=['DNI', 'Class'])
y = merge_data_sep['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

smote = SMOTE(random_state=42, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

print("Class distribution after using SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test_re, y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

perplexity_value = min(30, X_resampled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train_re, y_train_re)
lr_y_pred_re = lr_model.predict(X_test_re)

print(accuracy_score(y_test_re, lr_y_pred_re))
print(classification_report(y_test_re, lr_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test_re, lr_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train_re, y_train_re)

svc_y_pred_re = svc_model.predict(X_test_re)

print(accuracy_score(y_test_re, svc_y_pred_re))
print(classification_report(y_test_re, svc_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test_re, svc_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train_re, y_train_re)

k_y_pred_re = k_model.predict(X_test_re)

print(accuracy_score(y_test_re, k_y_pred_re))
print(classification_report(y_test_re, k_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test_re, k_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""### Data classification with SMOTE (creating 20 patients for each class)"""

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from collections import Counter

X = merge_data_sep.drop(columns=['DNI', 'Class'])
y = merge_data_sep['Class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

desired_samples = 20

sampling_strategy = {
    0: desired_samples,
    1: desired_samples,
    2: desired_samples,
    3: desired_samples
}

smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy, k_neighbors=1)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

inv_class_mapping = {v: k for k, v in class_mapping.items()}
y_resampled_labels = [inv_class_mapping[i] for i in y_resampled]

print("Class distribution after using SMOTE:", Counter(y_resampled_labels))

X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_resampled, y_resampled_labels, test_size=0.3, random_state=42)

model_re = RandomForestClassifier(n_estimators=200, max_depth=20, class_weight='balanced', random_state=42)
model_re.fit(X_train_re, y_train_re)

y_pred_re = model_re.predict(X_test_re)

print(classification_report(y_test_re, y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

rf_cm = confusion_matrix(y_test_re, y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('Random Forest Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt

importances = model_re.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.barh(np.array(X.columns)[sorted_idx], importances[sorted_idx])
plt.title("Gini Impurity")
plt.xlabel("Importance")
plt.ylabel("Characteristics")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

perplexity_value = min(30, X_resampled.shape[0] - 1)

tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
X_tsne = tsne.fit_transform(X_resampled)

tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'])
tsne_df['Class'] = y_resampled_labels

plt.figure(figsize=(10, 6))
sns.scatterplot(data=tsne_df, x='TSNE1', y='TSNE2', hue='Class', palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization (after SMOTE)')
plt.legend(title='Apnea Class')
plt.tight_layout()
plt.show()

"""#### AI models"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train_re, y_train_re)
lr_y_pred_re = lr_model.predict(X_test_re)

print(accuracy_score(y_test_re, lr_y_pred_re))
print(classification_report(y_test_re, lr_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

lr_cm = confusion_matrix(y_test_re, lr_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.svm import SVC

svc_model = SVC(kernel='rbf')
svc_model.fit(X_train_re, y_train_re)

svc_y_pred_re = svc_model.predict(X_test_re)

print(accuracy_score(y_test_re, svc_y_pred_re))
print(classification_report(y_test_re, svc_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

svc_cm = confusion_matrix(y_test_re, svc_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(svc_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

from sklearn.neighbors import KNeighborsClassifier

k_model = KNeighborsClassifier(n_neighbors=5)
k_model.fit(X_train_re, y_train_re)

k_y_pred_re = k_model.predict(X_test_re)

print(accuracy_score(y_test_re, k_y_pred_re))
print(classification_report(y_test_re, k_y_pred_re))

from sklearn.metrics import confusion_matrix
import seaborn as sns

k_cm = confusion_matrix(y_test_re, k_y_pred_re)

plt.figure(figsize=(8, 6))
sns.heatmap(k_cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.title('KNeighbors Confusion Matrix')
plt.show()

"""## Result comparison

![image.png](data:image/png;base64,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)
"""